{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd4HFBKe5JV_"
      },
      "source": [
        "# Importing  necessary liberaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAtnSqNQAPdh"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugIDizZP6AkW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,year, month, dayofmonth, date_format,hour\n",
        "from pyspark.sql.types import *\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "kLtj2l5xAH3Z",
        "outputId": "e9040d00-d496-4042-ff6b-82d0b5287b5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79256c25f070>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://38eb64f1bcd7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>NYC Taxi Fare Predction</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# creating the spark session\n",
        "spark=SparkSession.builder.appName(\"NYC Taxi Fare Predction\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrTe2-2XpZlJ"
      },
      "outputs": [],
      "source": [
        "# spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HQkivv-5JfE"
      },
      "source": [
        "# Data Loading and Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3okKAyw0C9B1",
        "outputId": "22d02da0-68e3-4f16-8801-b6c2af729f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waiNqK-K8woa"
      },
      "outputs": [],
      "source": [
        "# variables\n",
        "zip_path = \"/content/drive/MyDrive/NYC_taxi_data/new-york-city-taxi-fare-prediction.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/NYC_taxi_data/\"\n",
        "train_file_path = \"/content/drive/MyDrive/NYC_taxi_data/train.csv\"\n",
        "test_file_path = \"/content/drive/MyDrive/NYC_taxi_data/test.csv\"\n",
        "number_of_rows = 200000\n",
        "min_latitude = -180\n",
        "max_latitude = 180\n",
        "min_longitude = -90\n",
        "max_longitude = 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zz4tezxNn4O"
      },
      "outputs": [],
      "source": [
        "# function to extract zip file\n",
        "def unzip_file(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        print(\"File unzipped successfully\")\n",
        "\n",
        "# unzip_file(zip_path, extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO_1syW3C_5g",
        "outputId": "c45c2026-9a10-4ead-b45d-ca823eede3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP-Coupons-Instructions.rtf\t\tsample_submission.csv  train.csv\n",
            "new-york-city-taxi-fare-prediction.zip\ttest.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/NYC_taxi_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZocIaO4D3cQ",
        "outputId": "42e11857-2dee-49c7-e82b-a798400a54de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
            "|                key|fare_amount|    pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
            "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
            "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:21|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|\n",
            "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:16|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
            "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:00|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
            "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:42|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|\n",
            "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:00|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|\n",
            "|2011-01-06 09:50:45|       12.1|2011-01-06 09:50:45|      -74.000964|       40.73163|       -73.972892|       40.758233|              1|\n",
            "|2012-11-20 20:35:00|        7.5|2012-11-20 20:35:00|      -73.980002|      40.751662|       -73.973802|       40.764842|              1|\n",
            "|2012-01-04 17:22:00|       16.5|2012-01-04 17:22:00|        -73.9513|      40.774138|       -73.990095|       40.751048|              1|\n",
            "|2012-12-03 13:10:00|        9.0|2012-12-03 13:10:00|      -74.006462|      40.726713|       -73.993078|       40.731628|              1|\n",
            "|2009-09-02 01:11:00|        8.9|2009-09-02 01:11:00|      -73.980658|      40.733873|        -73.99154|       40.758138|              2|\n",
            "+-------------------+-----------+-------------------+----------------+---------------+-----------------+----------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# read csv file from google drive and show 10 rows from the dataframe\n",
        "df = spark.read.csv(train_file_path, header=True, inferSchema=True).limit(number_of_rows)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pg7amvTEKrw",
        "outputId": "24325108-f44b-4723-828d-1fb960966930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- key: timestamp (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- pickup_longitude: double (nullable = true)\n",
            " |-- pickup_latitude: double (nullable = true)\n",
            " |-- dropoff_longitude: double (nullable = true)\n",
            " |-- dropoff_latitude: double (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check for the columns, data types and if there are any null values\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpu0_dGREyGN",
        "outputId": "fb4a3cee-03bb-4eef-fb63-86a54f74964a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 1\n"
          ]
        }
      ],
      "source": [
        "# check for the number of partitons\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "print(\"Number of partitions:\", num_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFrH42x8FqvM",
        "outputId": "a6f54574-aee4-48b7-8293-5f8b73a34fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 200000\n"
          ]
        }
      ],
      "source": [
        "# count number of rows\n",
        "row_count = df.count()\n",
        "print(\"Number of rows:\", row_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjeDPYuBGvrh",
        "outputId": "8263e725-3c1d-49e8-eb02-ff72e4429050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
            "|summary|       fare_amount|  pickup_longitude|   pickup_latitude| dropoff_longitude| dropoff_latitude|   passenger_count|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
            "|  count|            200000|            200000|            200000|            199999|           199999|            200000|\n",
            "|   mean|11.342876950000601|-72.50612144955218|39.922325777255104|-72.51867346221218|39.92557945791384|          1.682445|\n",
            "| stddev| 9.837854787330032|11.608096802996164|10.048946659938037|10.724225862534897|6.751120031010652|1.3067296429203594|\n",
            "|    min|             -44.9|           -736.55|      -3116.285383|       -1251.19589|      -1189.61544|                 0|\n",
            "|    max|             500.0|        2140.60116|       1703.092772|         40.851027|       404.616667|                 6|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking for the statistical summery\n",
        "summary_df =df.describe()\n",
        "summary_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isJpOWYsRlgp"
      },
      "source": [
        "**The minimum** fare is in negative, which is clearly erroneous (fares should not be negative).\n",
        "**The maximum** fare is $93,963.36, which is extremely high and likely an outlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuvmqkR55Jhc"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl0Un7JqSQmj"
      },
      "outputs": [],
      "source": [
        "# drop the rows with the null columns\n",
        "df_new=df.na.drop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ajrvp5guGfPl"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "df_new=df_new.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvBAxuQ8GfZz"
      },
      "outputs": [],
      "source": [
        "# filtering out the erroneous data\n",
        "df_new = df_new.filter(col('fare_amount') >= 0) \\\n",
        "               .filter((col('passenger_count') > 0) & (col('passenger_count') < 7)) \\\n",
        "               .filter((col('pickup_longitude') >= min_latitude) & (col('pickup_longitude') <= max_latitude)) \\\n",
        "               .filter((col('pickup_latitude') >= min_longitude) & (col('pickup_latitude') <= max_longitude)) \\\n",
        "               .filter((col('dropoff_longitude') >= min_latitude) & (col('dropoff_longitude') <= max_latitude)) \\\n",
        "               .filter((col('dropoff_latitude') >= min_longitude) & (col('dropoff_latitude') <= max_longitude))\n",
        "\n",
        "# alternatively, df_new = df_new.where(col('fare_amount') >= 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF3Xt0FHtCfA"
      },
      "outputs": [],
      "source": [
        "# removeing outlier\n",
        "q1 = df_new.approxQuantile(\"fare_amount\", [0.25], 0.01)[0]\n",
        "q3 = df_new.approxQuantile(\"fare_amount\", [0.75], 0.01)[0]\n",
        "iqr = q3 - q1\n",
        "\n",
        "# defining upper and the lower bound\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiTDIgmotJzl"
      },
      "outputs": [],
      "source": [
        "df_new = df_new.filter(col('fare_amount') >= lower_bound) \\\n",
        "               .filter(col('fare_amount') <= upper_bound )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U4HCi8uGfk0",
        "outputId": "32914a52-6877-42ea-c76e-10f684a4202d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- pickup_longitude: double (nullable = true)\n",
            " |-- pickup_latitude: double (nullable = true)\n",
            " |-- dropoff_longitude: double (nullable = true)\n",
            " |-- dropoff_latitude: double (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- hour_of_day: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# extracting new columns from pickup_datetime\n",
        "df_new = df_new.withColumn(\"year\", year(col(\"pickup_datetime\"))) \\\n",
        "       .withColumn(\"month\", month(col(\"pickup_datetime\"))) \\\n",
        "       .withColumn(\"day\", dayofmonth(col(\"pickup_datetime\"))) \\\n",
        "       .withColumn(\"day_of_week\", date_format(col(\"pickup_datetime\"), 'u').cast(IntegerType())) \\\n",
        "       .withColumn(\"hour_of_day\", hour(col(\"pickup_datetime\"))) \\\n",
        "       .drop(\"key\")\n",
        "\n",
        "df_new.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "_2MohoNjjjC4",
        "outputId": "779e1f1a-9e6b-4375-a2b8-8b0053b1e1d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SparkUpgradeException",
          "evalue": "[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'u' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSparkUpgradeException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d3947d0a0398>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSparkUpgradeException\u001b[0m: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'u' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'."
          ]
        }
      ],
      "source": [
        "df_new.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hLVPC6aiu2V"
      },
      "outputs": [],
      "source": [
        "# create tempurarty view to execute sql queries\n",
        "df_new.createOrReplaceTempView(\"NYC_taxi_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAu1OILmnUHa"
      },
      "outputs": [],
      "source": [
        "# Mean, median, and standard deviation for numerical columns\n",
        "summery_stats = \"\"\"\n",
        "SELECT\n",
        "    AVG(fare_amount) AS avg_fare_amount,\n",
        "    PERCENTILE_APPROX(fare_amount, 0.5) AS median_fare_amount,\n",
        "    STDDEV(fare_amount) AS stddev_fare_amount,\n",
        "    AVG(pickup_longitude) AS avg_pickup_longitude,\n",
        "    AVG(pickup_latitude) AS avg_pickup_latitude,\n",
        "    AVG(dropoff_longitude) AS avg_dropoff_longitude,\n",
        "    AVG(dropoff_latitude) AS avg_dropoff_latitude,\n",
        "    AVG(passenger_count) AS avg_passenger_count\n",
        "FROM NYC_taxi_data\n",
        "\"\"\"\n",
        "summary_stats = spark.sql(summery_stats)\n",
        "summary_stats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGK92oSlovsa"
      },
      "outputs": [],
      "source": [
        "# which time of the day is the busiest\n",
        "busiest_hour = \"\"\"\n",
        "SELECT\n",
        "    hour_of_day,\n",
        "    COUNT(*) AS trip_count\n",
        "FROM NYC_taxi_data\n",
        "GROUP BY hour_of_day\n",
        "ORDER BY trip_count DESC\n",
        "\"\"\"\n",
        "busiest_hour = spark.sql(busiest_hour)\n",
        "busiest_hour.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkBCPR7zqlj8"
      },
      "outputs": [],
      "source": [
        "# which day of the week is the busiest day\n",
        "busiest_day_of_week = \"\"\"\n",
        "SELECT\n",
        "    day_of_week,\n",
        "    COUNT(*) AS trip_count\n",
        "FROM NYC_taxi_data\n",
        "GROUP BY day_of_week\n",
        "ORDER BY trip_count DESC\n",
        "\"\"\"\n",
        "busiest_day_of_week = spark.sql(busiest_day_of_week)\n",
        "busiest_day_of_week.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk8fKUb6rfgK"
      },
      "outputs": [],
      "source": [
        "# daily trend: average fare amount and trip count per day\n",
        "daily_trends = spark.sql(\"\"\"\n",
        "    SELECT year, month, day,\n",
        "           AVG(fare_amount) AS average_fare,\n",
        "           COUNT(*) AS trip_count\n",
        "    FROM taxi_data\n",
        "    GROUP BY year, month, day\n",
        "    ORDER BY year, month, day\n",
        "\"\"\")\n",
        "daily_trends.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weekly average fare and trip count by day of the week\n",
        "weekly_trends = spark.sql(\"\"\"\n",
        "    SELECT day_of_week,\n",
        "           AVG(fare_amount) AS average_fare,\n",
        "           COUNT(*) AS trip_count\n",
        "    FROM taxi_data\n",
        "    GROUP BY day_of_week\n",
        "    ORDER BY day_of_week\n",
        "\"\"\")\n",
        "weekly_trends.show()\n"
      ],
      "metadata": {
        "id": "Y71kV05s3jiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C8bwEkQ5Jj3"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFWgubm85JnX"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoWF6hMB5mm6"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select features and target variable\n",
        "feature_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
        "                   \"passenger_count\", \"year\", \"month\", \"day\", \"day_of_week\", \"hour_of_day\"]\n",
        "target_column = \"fare_amount\""
      ],
      "metadata": {
        "id": "iZWPb-Zm4r5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets (70% training, 30% testing)\n",
        "train_data, test_data = df_new.randomSplit([0.7, 0.3], seed=123)"
      ],
      "metadata": {
        "id": "25T3IZSb497e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df_new = assembler.transform(df_new)"
      ],
      "metadata": {
        "id": "Q2NGgjZ05A8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VectorAssembler to combine feature columns\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
      ],
      "metadata": {
        "id": "2AZ5O7DU5QLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3a_bHeJ5mpN"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqVJPzl5msq"
      },
      "source": [
        "# Model Selection and Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=target_column)\n"
      ],
      "metadata": {
        "id": "WtK8pcAp5ZE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the parameter grid for Linear Regression\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n"
      ],
      "metadata": {
        "id": "Nt61ZsAf62EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the evaluator\n",
        "evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# det up cross-validation\n",
        "crossval = CrossValidator(\n",
        "    estimator=Pipeline(stages=[assembler, lr]),  # the pipeline with assembler and linear regression\n",
        "    estimatorParamMaps=paramGrid,  # parameter grid to search\n",
        "    evaluator=evaluator,  # evaluation metric\n",
        "    numFolds=3  # three-fold cross-validation\n",
        ")\n"
      ],
      "metadata": {
        "id": "bGHBRKpC692m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "cv_model = crossval.fit(train_data)"
      ],
      "metadata": {
        "id": "TKdBwKbI5iqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions from the best model\n",
        "predictions = cv_model.transform(test_data)\n",
        "\n",
        "# evaluate the best model using RMSE\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Best Model's RMSE on test data: {rmse}\")\n",
        "\n",
        "# retrieve the best model's parameters\n",
        "best_model = cv_model.bestModel.stages[-1]\n",
        "print(\"Best Model's regParam:\", best_model._java_obj.getRegParam())\n",
        "print(\"Best Model's elasticNetParam:\", best_model._java_obj.getElasticNetParam())"
      ],
      "metadata": {
        "id": "0h9mMqF_5oO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De5xFR5N5vxw"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7kwF31h5vzo"
      },
      "source": [
        "# Final Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXrSVs0O5v3J"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D43hHMKW5gZb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}